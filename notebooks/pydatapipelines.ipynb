{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating verbs for data pipelines\n",
    "\n",
    "Since a few years, pipelines (via `%>%` of the [magrittr package](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)) are quite popular in R and the grown ecosystem of the [\"tidyverse\"](https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/) is built around pipelines. Having tried both the pandas syntax (e.g. chaining like `df.groupby().mean()` or plain `function2(function1(input))`) and the R's pipeline syntax, I have to admit that I like the pipeline syntax a lot more.\n",
    "\n",
    "In my opinion the strength of R's pipeline syntax is:\n",
    "\n",
    "* The **same verbs can be used for different inputs** (there are [SQL backends for dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/new-sql-backend.html)), thanks to R's single-dispatch mechanism (called [S3 objects](http://adv-r.had.co.nz/S3.html)). \n",
    "* Thanks to **using function** instead of class methods, it's also more easily extendable (for a new method on `pandas.DataFrame` you have to add that to the pandas repository or you need to use monkey patching). Fortunatelly, both functions and singledispatch are also available in python :-)\n",
    "* It **uses normal functions** as pipline parts: `input %>% function()` is equivalent to `function(input)`. Unfortunately, this isn't easily matched in python, as pythons evaluation rules would first evaluate `function()` (e.g. call functions without any input). So one has to make `function()` return a helper object which can then be used as a pipeline part.\n",
    "* R's delayed evaluation rules make it easy to **evaluate arguments in the context of the pipeline**, e.g. `df %>% select(x)` would be converted to the equivalent of pandas `df[[\"x\"]]`, e.g. the name of the variable will be used in the selection. In python it would either error (if `x` is not defined) or (if `x` was defined, e.g. `x = \"column\"`), would take the value of `x`, e.g. `df[[\"column\"]]`. For this, some workarounds exist by using helper objects like `select(X.x)`, e.g. [pandas-ply and its `Symbolic expression`](https://github.com/coursera/pandas-ply).\n",
    "\n",
    "There exist a few implementation of dplyr like pipeline verbs for python (e.g. [pandas itself](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pipe.html), [pandas-ply](https://github.com/coursera/pandas-ply) (uses method chaining instead of a pipe operator), [dplython](https://github.com/dodger487/dplython), and [dfply](https://github.com/kieferk/dfply)), but they all focus on implementing dplyr style pipelines for `pandas.DataFrames` and I wanted to try out a simpler but more general approach to pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code\n",
    "\n",
    "The following shows my take on how to implement the first three things (I left out \"Symbolic expressions\"). The code is available in https://github.com/janschulz/pydatapipes. The short (removed the docstrings) version is actually only a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import singledispatch, wraps\n",
    "\n",
    "class PipeVerb():\n",
    "    \"\"\"Object which represents a part of a pipeline\"\"\"\n",
    "    def __init__(self, func, *args, **kwargs):\n",
    "        self.pipe_func = func\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __rrshift__(self, input):\n",
    "        return self.pipe_func(input, *self.args, **self.kwargs)\n",
    "\n",
    "\n",
    "def pipeverb(func):\n",
    "    \"\"\"Decorator to convert a function to a pipeline verb (without singledispatch)\"\"\"\n",
    "    @wraps(func)\n",
    "    def decorated(*args, **kwargs):\n",
    "        return PipeVerb(func, *args, **kwargs)\n",
    "    \n",
    "    # If it is a singledispatch method, expose the register method here as well\n",
    "    if hasattr(func, 'register'):\n",
    "        decorated.register = func.register\n",
    "\n",
    "    return decorated\n",
    "\n",
    "\n",
    "def make_pipesource(cls):\n",
    "    \"\"\"Enables a class to function as a pipe source\"\"\"\n",
    "    if hasattr(cls, '__rshift__') and (not getattr(cls.__rshift__, 'pipeoperator', False)):\n",
    "        def __rshift__(self, other):\n",
    "            \"\"\"Pipeline operator if the right side is a PipeVerb\"\"\"\n",
    "            if isinstance(other, PipeVerb):\n",
    "                return other.__rrshift__(self)\n",
    "            else:\n",
    "                return self.__orig_rshift__(other)\n",
    "\n",
    "        cls.__orig_rshift__ = cls.__rshift__\n",
    "        cls.__rshift__ = __rshift__\n",
    "        setattr(cls.__rshift__, \"pipeoperator\", True)\n",
    "\n",
    "\n",
    "def singledispatch_pipeverb(func):\n",
    "    \"\"\"Convenience decorator to convert a function to a singledispatch pipeline verb\"\"\"\n",
    "    return pipeverb(singledispatch(func))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple pipeline verbs\n",
    "\n",
    "For end users wanting to build a new pipeline verb or add pipeline functionality to a new data source,\n",
    "there are two functions to build new pipeline parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pydatapipes.pipes import singledispatch_pipeverb, make_pipesource\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generic version which defines the API and should raise NotImplementedError\n",
    "@singledispatch_pipeverb\n",
    "def append_col(input, x = 1):\n",
    "    \"\"\"Appends x to the data source\"\"\"\n",
    "    raise NotImplementedError(\"append_col is not implemented for data of type %s\" % type(input))\n",
    "\n",
    "# concrete implementation for pandas.DataFrame\n",
    "@append_col.register(pd.DataFrame)\n",
    "def append_col_df(input, x = 1):\n",
    "    # always ensure that you return new data!\n",
    "    copy = input.copy()\n",
    "    copy[\"X\"] = x\n",
    "    return copy\n",
    "\n",
    "# ensure that pd.DataFrame is useable as a pipe source\n",
    "make_pipesource(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can then be used in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  X\n",
      "0  1  3\n",
      "1  2  3\n",
      "2  3  3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame({\"a\" : [1,2,3]}) >> append_col(x=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example implements a pipeline verb for `pandas.DataFrame`, but due to the useage of\n",
    "`singledispatch`, this is generic. By implementing additional\n",
    "`append_col_<data_source_type>()` functions and registering it with the original `append_col` function,\n",
    "the `append_col` function can be used with other data sources, e.g. SQL databases, HDF5, or even builtin data\n",
    "types like `list` or `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@append_col.register(list)\n",
    "def append_col_df(input, x = 1):\n",
    "    return input + [x]\n",
    "\n",
    "[1, 2] >> append_col()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a verb has no actual implementation for a data source, it will simply raise an `NotImplementedError`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append_col is not implemented for data of type <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    1 >> append_col()\n",
    "except NotImplementedError as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example: grouped and ungrouped aggregation on a pandas DataFrame\n",
    "\n",
    "`singledispatch` also makes it easy to work with grouped and ungrouped `pd.DataFrame`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@singledispatch_pipeverb\n",
    "def groupby(input, columns):\n",
    "    \"\"\"Group the input by columns\"\"\"\n",
    "    raise NotImplementedError(\"groupby is not implemented for data of type %s\" % type(input))\n",
    "\n",
    "@groupby.register(pd.DataFrame)\n",
    "def groupby_DataFrame(input, columns):\n",
    "    \"\"\"Group a DataFrame\"\"\"\n",
    "    return input.groupby(columns)    \n",
    "    \n",
    "@singledispatch_pipeverb\n",
    "def summarize_mean(input):\n",
    "    \"\"\"Summarize the input via mean aggregation\"\"\"\n",
    "    raise NotImplementedError(\"summarize_mean is not implemented for data of type %s\" % type(input))\n",
    "\n",
    "@summarize_mean.register(pd.DataFrame)\n",
    "def summarize_mean_DataFrame(input):\n",
    "    \"\"\"Summarize a DataFrame via mean aggregation\"\"\"\n",
    "    return input.mean()\n",
    "\n",
    "@summarize_mean.register(pd.core.groupby.GroupBy)\n",
    "def summarize_mean_GroupBy(input):\n",
    "    \"\"\"Summarize a grouped DataFrame via mean aggregation\"\"\"\n",
    "    return input.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"a\" : [1, 2, 3, 4], \"b\": [1, 1, 2, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    2.5\n",
      "b    1.5\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df >> summarize_mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a\n",
      "b     \n",
      "1  1.5\n",
      "2  3.5\n"
     ]
    }
   ],
   "source": [
    "print(df >> groupby(\"b\") >> summarize_mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitiations\n",
    "\n",
    "Compared to R's implementation in the [magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) package, \n",
    "`input >> verb(x)` can't be used as `verb(input, x)`. \n",
    "\n",
    "The problem here is that `verb(x)` under the hood constructs a helper object (`PipeVerb`) which \n",
    "is used in the rshift operation. At the time of calling `verb(...)`, we can't always be sure \n",
    "whether we want an object which can be used in the pipeline or already\n",
    "compute the result. As an example consider a verb `merge(*additional_data)`. You could call\n",
    "that as `data >> merge(first, second)` to indicate that you want all three (`data`,\n",
    "`first`, and `second`) merged. On the other hand, `merge(first, second)` is also valid\n",
    "(\"merge `first` and `second` together)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage as function and pipeline verb\n",
    "To help work around this problem, the convenience decorator `singledispatch_pipeverb` is actually not the best option if \n",
    "you want to create reuseable pipline verbs. Instead, the `singledispatch_pipeverb` decorator is also available in\n",
    "two parts, so that one can both expose the original function (with `singledispatch` enabled) and the\n",
    "final pipeline verb version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pydatapipes.pipes import pipeverb, singledispatch\n",
    "\n",
    "# first use singledispatch on the original function, but define it with a trailing underscore\n",
    "@singledispatch\n",
    "def my_verb_(input, x=1, y=2):\n",
    "    raise NotImplemented(\"my_verb is not implemented for data of type %s\" % type(input))\n",
    "\n",
    "# afterwards convert the original function to the pipeline verb:\n",
    "my_verb = pipeverb(my_verb_)\n",
    "\n",
    "# concrete implementations can be registered on both ``my_verb`` and ``my_verb_``\n",
    "@my_verb_.register(list)\n",
    "def my_verb_df(input, x=1, y=2):\n",
    "    return input + [x, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A user can now use both versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] >> my_verb(x=2, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_verb_([9], x=2, y=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules and conventions\n",
    "\n",
    "To work as a pipline verb, functions **must** follow these rules:\n",
    "\n",
    "* Pipelines assume that the verbs itself are side-effect free, i.e. they do not change the inputs of \n",
    "  the data pipeline. This means that actual implementations of a verb for a specific data source \n",
    "  must ensure that the input is not changed in any way, e.g. if you want to pass on a changed value\n",
    "  of a `pd.DataFrame`, make a copy first.\n",
    "* The initial function (not the actual implementations for a specific data source) should usually\n",
    "  do nothing but simply raise `NotImplementedError`, as it is called for all other types of data\n",
    "  sources. \n",
    "\n",
    "The strength of the tidyverse is it's coherent API design. To ensure a coherent API for pipeline verbs, \n",
    "it would be nice if verbs would follow these conventions:\n",
    "\n",
    "* Pipeline verbs should actually be named as verbs, e.g. use `input >> summarize()` instead of\n",
    "  `input >> Summary()`\n",
    "* If you expose both the pipeline verb and a normal function (which can be called directly), \n",
    "  the pipeline verb should get the \"normal\" verb name and the function version should get \n",
    "  an underscore `_` appended: `x >> verb()` -> `verb_(x)`\n",
    "* The actual implementation function of a `verb()` for a data source of class `Type`\n",
    "  should be called `verb_Type(...)`, e.g. `select_DataFrame()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing parts\n",
    "\n",
    "So what is missing? Quite a lot :-)\n",
    "\n",
    "* Symbolic expressions: e.g. `select(X.x)` instead of `select(\"x\")`\n",
    "* Helper for dplyr style column selection (e.g. `select(starts_with(\"y2016_\"))` and `select(X[X.first_column:X.last_column])`)\n",
    "* all the dplyr, tidyr, ... verbs which make the tidyverse so great\n",
    "\n",
    "Some of this is already implemented in the other dplyr like python libs ([pandas-ply](https://github.com/coursera/pandas-ply), [dplython](https://github.com/dodger487/dplython), and [dfply](https://github.com/kieferk/dfply)), so I'm not sure how to go on. I really like my versions of pipelines but duplicating the works of them feels like a waste of time. So my next step is seeing if it's possible to integrate this with one of these solutions, probably dfply as that looks the closest implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "~env (conda_dev_35)",
   "language": "python",
   "name": "conda_dev_35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
